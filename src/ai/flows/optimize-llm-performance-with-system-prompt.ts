'use server';

/**
 * @fileOverview A flow for optimizing LLM performance with system prompts.
 *
 * - optimizeLLMPerformance - A function that optimizes LLM performance.
 * - OptimizeLLMPerformanceInput - The input type for the optimizeLLMPerformance function.
 * - OptimizeLLMPerformanceOutput - The return type for the optimizeLLMPerformance function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const OptimizeLLMPerformanceInputSchema = z.object({
  question: z.string().describe('The quiz question to be answered.'),
  context: z.string().optional().describe('Additional context for answering the question.'),
});
export type OptimizeLLMPerformanceInput = z.infer<typeof OptimizeLLMPerformanceInputSchema>;

const OptimizeLLMPerformanceOutputSchema = z.object({
  answer: z.string().describe('The answer generated by the LLM.'),
  reasoning: z.string().optional().describe('The reasoning behind the generated answer.'),
});
export type OptimizeLLMPerformanceOutput = z.infer<typeof OptimizeLLMPerformanceOutputSchema>;

export async function optimizeLLMPerformance(input: OptimizeLLMPerformanceInput): Promise<OptimizeLLMPerformanceOutput> {
  return optimizeLLMPerformanceFlow(input);
}

const prompt = ai.definePrompt({
  name: 'optimizeLLMPerformancePrompt',
  input: {schema: OptimizeLLMPerformanceInputSchema},
  output: {schema: OptimizeLLMPerformanceOutputSchema},
  prompt: `You are an AI quiz solver expert. Your goal is to answer the question as accurately as possible using the provided context.

Question: {{{question}}}

Context: {{{context}}}

Answer:`,
});

const optimizeLLMPerformanceFlow = ai.defineFlow(
  {
    name: 'optimizeLLMPerformanceFlow',
    inputSchema: OptimizeLLMPerformanceInputSchema,
    outputSchema: OptimizeLLMPerformanceOutputSchema,
  },
  async input => {
    const {output} = await prompt(input);
    return output!;
  }
);
